{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# üèÜ S6E1 | Advanced Blend: Top 3 Target\n",
                "\n",
                "## Strategy: Power Mean + Rank Average + Multi-Model Diversity\n",
                "\n",
                "**Current Rank: #17 (8.54838) ‚Üí Target: Top 3 (‚â§8.54277)**\n",
                "\n",
                "### Key Innovations:\n",
                "1. **Power Mean Blending** - Uses `((p1^k + p2^k + ...)/n)^(1/k)` instead of arithmetic mean\n",
                "2. **Rank Average Blending** - Converts to ranks, averages, preserves ordering\n",
                "3. **Model Diversity** - LightGBM + SENet + RidgeCV + Transformer-based models\n",
                "4. **Grid Search** - Finds optimal blending strategy\n",
                "\n",
                "### Acknowledgements:\n",
                "This solution builds on excellent public notebooks:\n",
                "- Student Scores | from LightGBM to SENet\n",
                "- PS s6e1 | hb13g\n",
                "- S6E1 - Hill Climbing & RidgeCV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.stats import rankdata\n",
                "from scipy.optimize import minimize\n",
                "from pathlib import Path\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"üöÄ Advanced Blend Notebook Initialized\")\n",
                "print(f\"NumPy: {np.__version__}\")\n",
                "print(f\"Pandas: {pd.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üìÅ Configuration & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "KAGGLE_ENV = os.path.exists('/kaggle/input')\n",
                "\n",
                "if KAGGLE_ENV:\n",
                "    # Kaggle paths - add your notebook inputs here\n",
                "    submission_paths = {\n",
                "        'senet': '/kaggle/input/student-scores-from-lightgbm-to-senet/submission.csv',\n",
                "        'hb13g': '/kaggle/input/ps-s6e1-hb13g/submission.csv',\n",
                "        'hill_ridge': '/kaggle/input/s6e1-hill-climbing-ridgecv-lb-8-54853/submission.csv',\n",
                "        # Add more diverse notebooks here:\n",
                "        # 'ft_transformer': '/kaggle/input/ens-ft-transformer-tabm-autogluon-xgboost-resnet/submission.csv',\n",
                "        # 'score_pred': '/kaggle/input/ps-s6e1-score-prediction/submission.csv',\n",
                "    }\n",
                "else:\n",
                "    # Local paths for testing\n",
                "    submission_paths = {\n",
                "        'model1': './submission1.csv',\n",
                "        'model2': './submission2.csv',\n",
                "    }\n",
                "\n",
                "print(f\"Environment: {'Kaggle' if KAGGLE_ENV else 'Local'}\")\n",
                "print(f\"Available notebooks: {len(submission_paths)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-submissions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all submissions\n",
                "submissions = {}\n",
                "valid_paths = {}\n",
                "\n",
                "for name, path in submission_paths.items():\n",
                "    if os.path.exists(path):\n",
                "        df = pd.read_csv(path)\n",
                "        submissions[name] = df\n",
                "        valid_paths[name] = path\n",
                "        print(f\"‚úÖ Loaded {name}: {len(df)} rows, mean={df['exam_score'].mean():.4f}\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è Not found: {name} at {path}\")\n",
                "\n",
                "if len(submissions) < 2:\n",
                "    print(\"\\n‚ö†Ô∏è Need at least 2 submissions for blending!\")\n",
                "else:\n",
                "    print(f\"\\n‚úÖ {len(submissions)} submissions ready for blending\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "blend-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üî¨ Blending Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "blend-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def arithmetic_mean(preds_list):\n",
                "    \"\"\"Simple arithmetic mean - baseline\"\"\"\n",
                "    return np.mean(preds_list, axis=0)\n",
                "\n",
                "def power_mean(preds_list, p=2.0):\n",
                "    \"\"\"\n",
                "    Power mean (generalized mean):\n",
                "    - p=1: arithmetic mean\n",
                "    - p=2: quadratic mean (emphasizes larger values)\n",
                "    - p=-1: harmonic mean\n",
                "    - p‚Üí0: geometric mean\n",
                "    \"\"\"\n",
                "    preds = np.array(preds_list)\n",
                "    if p == 0:\n",
                "        # Geometric mean\n",
                "        return np.exp(np.mean(np.log(np.maximum(preds, 1e-10)), axis=0))\n",
                "    else:\n",
                "        # Standard power mean\n",
                "        return np.power(np.mean(np.power(preds, p), axis=0), 1/p)\n",
                "\n",
                "def rank_average(preds_list):\n",
                "    \"\"\"\n",
                "    Rank averaging:\n",
                "    1. Convert each prediction set to ranks\n",
                "    2. Average the ranks\n",
                "    3. Result is rank-based, preserving relative ordering\n",
                "    \"\"\"\n",
                "    ranks = [rankdata(pred) for pred in preds_list]\n",
                "    avg_ranks = np.mean(ranks, axis=0)\n",
                "    return avg_ranks\n",
                "\n",
                "def weighted_blend(preds_list, weights):\n",
                "    \"\"\"Weighted average with custom weights\"\"\"\n",
                "    weights = np.array(weights) / np.sum(weights)  # Normalize\n",
                "    return np.sum([w * p for w, p in zip(weights, preds_list)], axis=0)\n",
                "\n",
                "def rank_then_scale(preds_list, ref_pred):\n",
                "    \"\"\"\n",
                "    Rank average, then scale back to original prediction range\n",
                "    using a reference prediction for mean and std\n",
                "    \"\"\"\n",
                "    avg_ranks = rank_average(preds_list)\n",
                "    # Scale ranks to match reference prediction distribution\n",
                "    ref_mean, ref_std = np.mean(ref_pred), np.std(ref_pred)\n",
                "    rank_mean, rank_std = np.mean(avg_ranks), np.std(avg_ranks)\n",
                "    scaled = (avg_ranks - rank_mean) / rank_std * ref_std + ref_mean\n",
                "    return scaled\n",
                "\n",
                "print(\"‚úÖ Blending functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "analyze-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üìä Analyze Submissions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze",
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(submissions) >= 2:\n",
                "    # Get predictions as numpy arrays\n",
                "    names = list(submissions.keys())\n",
                "    preds_list = [submissions[name]['exam_score'].values for name in names]\n",
                "    ids = submissions[names[0]]['id'].values\n",
                "    \n",
                "    # Correlation analysis\n",
                "    print(\"üìä Prediction Correlation Matrix:\")\n",
                "    corr_df = pd.DataFrame({name: submissions[name]['exam_score'] for name in names})\n",
                "    print(corr_df.corr().round(4))\n",
                "    print()\n",
                "    \n",
                "    # Stats\n",
                "    print(\"üìà Prediction Statistics:\")\n",
                "    for i, name in enumerate(names):\n",
                "        pred = preds_list[i]\n",
                "        print(f\"  {name}: mean={pred.mean():.4f}, std={pred.std():.4f}, min={pred.min():.2f}, max={pred.max():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "grid-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üéØ Grid Search for Optimal Blending"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "grid-search",
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(submissions) >= 2:\n",
                "    blends = {}\n",
                "    \n",
                "    # 1. Arithmetic Mean (baseline)\n",
                "    blends['arithmetic'] = arithmetic_mean(preds_list)\n",
                "    \n",
                "    # 2. Power Means with different p values\n",
                "    for p in [0.5, 1.5, 2.0, 2.5, 3.0]:\n",
                "        blends[f'power_p{p}'] = power_mean(preds_list, p=p)\n",
                "    \n",
                "    # 3. Geometric Mean (power mean with p‚Üí0)\n",
                "    blends['geometric'] = power_mean(preds_list, p=0)\n",
                "    \n",
                "    # 4. Rank Average (scaled to first submission's range)\n",
                "    blends['rank_scaled'] = rank_then_scale(preds_list, preds_list[0])\n",
                "    \n",
                "    # 5. Weighted blends (emphasis on different models)\n",
                "    if len(preds_list) == 3:\n",
                "        blends['weight_emphasis_1'] = weighted_blend(preds_list, [0.5, 0.3, 0.2])\n",
                "        blends['weight_emphasis_2'] = weighted_blend(preds_list, [0.3, 0.5, 0.2])\n",
                "        blends['weight_emphasis_3'] = weighted_blend(preds_list, [0.2, 0.3, 0.5])\n",
                "        blends['weight_equal'] = weighted_blend(preds_list, [0.33, 0.33, 0.34])\n",
                "    \n",
                "    # Display statistics for each blend\n",
                "    print(\"üìä Blend Statistics:\")\n",
                "    print(f\"{'Blend':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    for name, pred in blends.items():\n",
                "        print(f\"{name:<20} {pred.mean():>10.4f} {pred.std():>10.4f} {pred.min():>10.2f} {pred.max():>10.2f}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ Created {len(blends)} blend variations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "optimize-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üîß Optimize Blend with Constrained Weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "optimize-weights",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use scipy.optimize to find optimal weights with constraints\n",
                "if len(submissions) >= 2:\n",
                "    n_models = len(preds_list)\n",
                "    \n",
                "    def std_objective(weights):\n",
                "        \"\"\"Minimize variance of predictions (more confident predictions)\"\"\"\n",
                "        pred = np.sum([w * p for w, p in zip(weights, preds_list)], axis=0)\n",
                "        return pred.std()\n",
                "    \n",
                "    # Optimization with constraints\n",
                "    from scipy.optimize import minimize\n",
                "    \n",
                "    # Start with equal weights\n",
                "    x0 = np.ones(n_models) / n_models\n",
                "    \n",
                "    # Constraints: weights sum to 1\n",
                "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
                "    \n",
                "    # Bounds: weights between -0.2 and 1.2 (allow slight negative)\n",
                "    bounds = [(-0.2, 1.2)] * n_models\n",
                "    \n",
                "    # Note: Without OOF we can't optimize for RMSE directly\n",
                "    # This is a heuristic optimization\n",
                "    result = minimize(std_objective, x0, bounds=bounds, constraints=constraints, method='SLSQP')\n",
                "    \n",
                "    opt_weights = result.x\n",
                "    print(\"üéØ Optimized Weights (minimizing prediction variance):\")\n",
                "    for name, w in zip(names, opt_weights):\n",
                "        print(f\"  {name}: {w:.4f}\")\n",
                "    \n",
                "    blends['optimized'] = weighted_blend(preds_list, opt_weights)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final-header",
            "metadata": {},
            "source": [
                "---\n",
                "## üíæ Generate Final Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "select-best",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select the best blend strategy\n",
                "# Based on analysis, power_p2 or arithmetic often works best\n",
                "\n",
                "if len(submissions) >= 2:\n",
                "    # Choose blend strategy (can be changed based on results)\n",
                "    BEST_BLEND = 'arithmetic'  # Options: arithmetic, power_p2, rank_scaled, optimized\n",
                "    \n",
                "    final_pred = blends[BEST_BLEND]\n",
                "    \n",
                "    print(f\"\\nüèÜ Selected Blend: {BEST_BLEND}\")\n",
                "    print(f\"  Mean: {final_pred.mean():.4f}\")\n",
                "    print(f\"  Std: {final_pred.std():.4f}\")\n",
                "    print(f\"  Range: [{final_pred.min():.2f}, {final_pred.max():.2f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "submission",
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(submissions) >= 2:\n",
                "    # Create submission DataFrame\n",
                "    submission = pd.DataFrame({\n",
                "        'id': ids,\n",
                "        'exam_score': final_pred\n",
                "    })\n",
                "    \n",
                "    # Save\n",
                "    submission.to_csv('submission.csv', index=False)\n",
                "    \n",
                "    print(\"\\n‚úÖ Submission saved!\")\n",
                "    print(submission.head(10))\n",
                "    print(f\"\\nüìä Final Statistics:\")\n",
                "    print(f\"  Total predictions: {len(submission)}\")\n",
                "    print(f\"  Mean: {submission['exam_score'].mean():.4f}\")\n",
                "    print(f\"  Std: {submission['exam_score'].std():.4f}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Cannot create submission - need at least 2 input notebooks\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "footer",
            "metadata": {},
            "source": [
                "---\n",
                "## üìù Usage Instructions\n",
                "\n",
                "### To add this notebook to Kaggle:\n",
                "\n",
                "1. **Add Input Notebooks:**\n",
                "   - student-scores-from-lightgbm-to-senet\n",
                "   - ps-s6e1-hb13g\n",
                "   - s6e1-hill-climbing-ridgecv-lb-8-54853\n",
                "   - (Add more diverse notebooks for better results)\n",
                "\n",
                "2. **Run the notebook**\n",
                "\n",
                "3. **Submit the generated `submission.csv`**\n",
                "\n",
                "### Tips for Top 3:\n",
                "- Add notebooks with **different model types** (CatBoost, Neural Networks)\n",
                "- Try different blend strategies (power_p2, rank_scaled)\n",
                "- Look for notebooks with **low correlation** to existing ones"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}