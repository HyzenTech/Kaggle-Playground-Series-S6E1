# üèÜ Kaggle Playground Series S6E1 - Predicting Student Test Scores

<div align="center">

[![Kaggle](https://img.shields.io/badge/Kaggle-Competition-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/playground-series-s6e1)
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

**ü•à Best Rank: #17 | üéØ Target: Top 3**

[Competition Link](https://www.kaggle.com/competitions/playground-series-s6e1) ‚Ä¢ [Leaderboard](https://www.kaggle.com/competitions/playground-series-s6e1/leaderboard)

</div>

---

## üìä Competition Overview

| Metric | Value |
|--------|-------|
| **Task** | Regression (Predict exam scores) |
| **Evaluation** | RMSE (Root Mean Squared Error) |
| **Train Size** | 195,469 samples |
| **Test Size** | 130,313 samples |
| **Features** | 14 (demographics, study habits, resources) |

---

## üèÖ Results & Progress

| Version | Description | CV RMSE | LB Score | Rank |
|---------|-------------|---------|----------|------|
| v1 | Baseline XGBoost | 8.85 | 8.72 | #400+ |
| v3 | Multi-model ensemble | 8.75 | 8.66 | #265 |
| v5 | Hill Climbing + Stacking | 8.71 | 8.55 | #50 |
| **v6** | **Advanced Blending (3 notebooks)** | - | **8.548** | **#17** |

---

## üìÅ Repository Structure

```
üì¶ Kaggle-Playground-Series-S6E1
‚îú‚îÄ‚îÄ üìÇ Code/
‚îÇ   ‚îú‚îÄ‚îÄ professional_solution.ipynb    # Full pipeline with 7 models
‚îÇ   ‚îú‚îÄ‚îÄ exact_v5_kaggle.ipynb          # Minimal v5 solution
‚îÇ   ‚îú‚îÄ‚îÄ run_aggressive_v5.py           # Local execution script
‚îÇ   ‚îî‚îÄ‚îÄ submission_v5.csv              # Latest submission
‚îú‚îÄ‚îÄ üìÇ Dataset/
‚îÇ   ‚îú‚îÄ‚îÄ train.csv                      # Training data
‚îÇ   ‚îú‚îÄ‚îÄ test.csv                       # Test data
‚îÇ   ‚îî‚îÄ‚îÄ sample_submission.csv          # Submission format
‚îú‚îÄ‚îÄ üî¨ blend_advanced_top3.ipynb       # Advanced blending notebook
‚îú‚îÄ‚îÄ üìã kernel-metadata-advanced.json   # Kaggle metadata
‚îî‚îÄ‚îÄ üìñ README.md
```

---

## üî¨ Solution Architecture

### Phase 1: Feature Engineering
```
Raw Features ‚Üí Transforms ‚Üí 50+ Engineered Features
```

| Category | Features |
|----------|----------|
| **Polynomial** | log, square, cube, sqrt of numeric features |
| **Trigonometric** | sin/cos transformations |
| **Interaction** | study √ó attendance, sleep √ó study |
| **Encoding** | Ordinal encoding for categoricals |
| **Magic Formula** | `5.91√óstudy + 0.35√óattendance + 1.42√ósleep + 4.78` |

### Phase 2: Multi-Model Training

| Model | Type | GPU/CPU | Key Params |
|-------|------|---------|------------|
| Ridge | Linear | CPU | Œ± from CV |
| ElasticNet | Linear | CPU | l1_ratio=0.5 |
| BayesianRidge | Bayesian | CPU | Default |
| ExtraTrees | Ensemble | CPU | n_estimators=200 |
| LightGBM | GBDT | CPU | lr=0.012, iters=4000 |
| XGBoost | GBDT | CUDA | lr=0.012, iters=4000 |
| CatBoost | GBDT | GPU | lr=0.012, iters=4000 |

### Phase 3: Ensemble Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    7 Base Models                        ‚îÇ
‚îÇ  Ridge ‚îÇ ElasticNet ‚îÇ Bayesian ‚îÇ Trees ‚îÇ LGB ‚îÇ XGB ‚îÇ CB ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   OOF Predictions   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº               ‚ñº               ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   Hill    ‚îÇ   ‚îÇ   Ridge   ‚îÇ   ‚îÇ  Simple   ‚îÇ
   ‚îÇ Climbing  ‚îÇ   ‚îÇ  Stacking ‚îÇ   ‚îÇ  Average  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Final Blend      ‚îÇ
              ‚îÇ   (0.7 HC + 0.3 RC) ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Phase 4: Advanced Blending (Top 3 Strategy)

The `blend_advanced_top3.ipynb` implements:

| Technique | Formula | Use Case |
|-----------|---------|----------|
| **Arithmetic Mean** | `(p‚ÇÅ + p‚ÇÇ + ... + p‚Çô) / n` | Baseline |
| **Power Mean** | `((p‚ÇÅ·µè + p‚ÇÇ·µè + ...)/n)^(1/k)` | Emphasize extremes |
| **Geometric Mean** | `(p‚ÇÅ √ó p‚ÇÇ √ó ... √ó p‚Çô)^(1/n)` | Multiplicative blend |
| **Rank Average** | Convert to ranks ‚Üí average | Different distributions |

---

## üöÄ Quick Start

### Option 1: Run on Kaggle (Recommended)

1. **Fork** the notebook: [S6E1 | Blend Top Public Notebooks](https://www.kaggle.com/code/muhammadhafizy/s6e1-blend-top-public-notebooks)
2. **Add inputs** (competition data is auto-added)
3. **Run All** and submit

### Option 2: Run Locally

```bash
# Clone repository
git clone https://github.com/HyzenTech/Kaggle-Playground-Series-S6E1.git
cd Kaggle-Playground-Series-S6E1

# Install dependencies
pip install numpy pandas scikit-learn lightgbm xgboost catboost

# Run solution
python Code/run_aggressive_v5.py

# Output: submission.csv
```

### Option 3: Advanced Blending

```bash
# On Kaggle, add these notebooks as inputs:
# - student-scores-from-lightgbm-to-senet
# - ps-s6e1-hb13g  
# - s6e1-hill-climbing-ridgecv-lb-8-54853

# Then run blend_advanced_top3.ipynb
```

---

## üìà Key Insights

### What Worked ‚úÖ

1. **Original Dataset Augmentation** - Using the source Kaggle dataset alongside competition data
2. **Negative Weights in Hill Climbing** - Allowing weights from -0.5 to 1.5 for error cancellation
3. **Magic Formula** - The linear relationship `5.91√óstudy + 0.35√óattendance + 1.42√ósleep + 4.78` captures 60%+ variance
4. **Multi-Notebook Blending** - Combining diverse public notebooks beats single-model training

### What Didn't Work ‚ùå

1. Heavy feature selection - Keeping all features worked better
2. Deep neural networks - GBDTs dominated on this dataset
3. Simple averaging without optimization

---

## üõ†Ô∏è Dependencies

```
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
lightgbm>=3.3.0
xgboost>=1.5.0
catboost>=1.0.0
scipy>=1.7.0
```

---

## üìö References & Acknowledgements

This solution builds upon excellent public notebooks:

- [Student Scores | from LightGBM to SENet](https://www.kaggle.com/code/ambrosm/student-scores-from-lightgbm-to-senet) by AmbrosM
- [PS s6e1 | hb13g](https://www.kaggle.com/code/hb13g/ps-s6e1-hb13g) by hb13g
- [S6E1 - Hill Climbing & RidgeCV](https://www.kaggle.com/code/thomastschinkel/s6e1-hill-climbing-ridgecv-lb-8-54853) by Thomas Tschinkel

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Made with ‚ù§Ô∏è for the Kaggle Community**

‚≠ê Star this repo if you found it helpful!

</div>
