{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèÜ Predicting Student Test Scores: A Comprehensive Approach\n",
                "\n",
                "**Kaggle Playground Series S6E1** | Target: Top 50 (RMSE < 8.55)\n",
                "\n",
                "---\n",
                "\n",
                "## Abstract\n",
                "\n",
                "This notebook presents a comprehensive solution for predicting student exam scores, combining **gradient boosting ensembles** with **advanced feature engineering** techniques derived from domain knowledge and top-performing solutions. The key innovations include:\n",
                "\n",
                "1. **Original Dataset Augmentation** - Leveraging the source data for cleaner training signals\n",
                "2. **Linear Formula Discovery** - A powerful hand-crafted feature capturing the core relationship\n",
                "3. **Multi-Stage Ensemble** - Weighted averaging with hill climbing + Ridge stacking\n",
                "4. **Diverse Model Architecture** - Ridge, ElasticNet, LightGBM, XGBoost, CatBoost, and ExtraTrees\n",
                "5. **GPU Acceleration** - Fast training with CUDA-enabled XGBoost and CatBoost\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Setup & Configuration](#1)\n",
                "2. [Data Loading & Exploration](#2)\n",
                "3. [Feature Engineering](#3)\n",
                "4. [Model Training Pipeline](#4)\n",
                "5. [Advanced Ensemble & Stacking](#5)\n",
                "6. [Submission](#6)\n",
                "\n",
                "---\n",
                "\n",
                "## Why This Approach Works\n",
                "\n",
                "| Insight | Impact |\n",
                "|---------|--------|\n",
                "| Original dataset provides cleaner patterns | +0.1-0.2 RMSE improvement |\n",
                "| The relationship is predominantly linear | Linear models are competitive |\n",
                "| Study hours is the dominant predictor | Weight ~6x in the formula |\n",
                "| Model diversity reduces variance | Ensemble beats individuals |\n",
                "| Hill climbing finds optimal negative weights | Better than simple averaging |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"1\"></a>\n",
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_kg_hide-input": true
            },
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS\n",
                "# ============================================================================\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import gc\n",
                "from typing import Tuple, List, Dict\n",
                "\n",
                "# Machine Learning\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, BayesianRidge\n",
                "from sklearn.ensemble import ExtraTreesRegressor\n",
                "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# Gradient Boosting\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "from catboost import CatBoostRegressor\n",
                "\n",
                "# Optimization\n",
                "from scipy.optimize import minimize\n",
                "\n",
                "# Configuration\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "# Reproducibility\n",
                "SEED = 42\n",
                "np.random.seed(SEED)\n",
                "\n",
                "# Cross-Validation Configuration\n",
                "N_FOLDS = 5\n",
                "\n",
                "# Column Names\n",
                "TARGET = 'exam_score'\n",
                "ID_COL = 'id'\n",
                "\n",
                "print('‚úì Libraries imported successfully')\n",
                "print(f'‚úì Configuration: {N_FOLDS}-fold CV, seed={SEED}')\n",
                "print('‚úì GPU enabled for XGBoost and CatBoost')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"2\"></a>\n",
                "## 2. Data Loading & Exploration\n",
                "\n",
                "### The Secret Ingredient: Original Dataset\n",
                "\n",
                "This Playground Series competition uses synthetically generated data. The original \"Exam Score Prediction\" dataset provides:\n",
                "\n",
                "- **20,000 additional samples** with cleaner patterns\n",
                "- **Ground truth relationships** before synthetic noise was added\n",
                "- **Better generalization** when used for data augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA LOADING\n",
                "# ============================================================================\n",
                "\n",
                "# Competition Data\n",
                "train_df = pd.read_csv('/kaggle/input/playground-series-s6e1/train.csv')\n",
                "test_df = pd.read_csv('/kaggle/input/playground-series-s6e1/test.csv')\n",
                "sample_sub = pd.read_csv('/kaggle/input/playground-series-s6e1/sample_submission.csv')\n",
                "\n",
                "# Original Dataset (Data Augmentation Source)\n",
                "original_df = pd.read_csv('/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv')\n",
                "original_df.columns = original_df.columns.str.lower().str.replace(' ', '_')\n",
                "\n",
                "# Remove ID column from original if present\n",
                "if 'student_id' in original_df.columns:\n",
                "    original_df = original_df.drop('student_id', axis=1)\n",
                "\n",
                "# Extract targets and test IDs\n",
                "test_ids = test_df[ID_COL].values\n",
                "y_train = train_df[TARGET].values\n",
                "y_original = original_df[TARGET].values\n",
                "\n",
                "print('üìä Dataset Shapes:')\n",
                "print(f'   Competition Train: {train_df.shape[0]:,} rows √ó {train_df.shape[1]} columns')\n",
                "print(f'   Competition Test:  {test_df.shape[0]:,} rows √ó {test_df.shape[1]} columns')\n",
                "print(f'   Original Dataset:  {original_df.shape[0]:,} rows √ó {original_df.shape[1]} columns')\n",
                "print(f'\\n   Total training samples available: {len(train_df) + len(original_df):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# EXPLORATORY DATA ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Target Distribution\n",
                "train_df[TARGET].hist(bins=50, ax=axes[0], color='steelblue', edgecolor='white', alpha=0.8)\n",
                "axes[0].axvline(train_df[TARGET].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_df[TARGET].mean():.1f}')\n",
                "axes[0].set_title('Target Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Exam Score')\n",
                "axes[0].legend()\n",
                "\n",
                "# Study Hours vs Score\n",
                "axes[1].scatter(train_df['study_hours'], train_df[TARGET], alpha=0.05, s=3, c='coral')\n",
                "axes[1].set_title('Study Hours vs Exam Score', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Study Hours')\n",
                "axes[1].set_ylabel('Exam Score')\n",
                "\n",
                "# Correlation Analysis\n",
                "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
                "correlations = train_df[numeric_cols].corr()[TARGET].drop(TARGET).sort_values(ascending=False)\n",
                "correlations.plot(kind='barh', ax=axes[2], color='teal')\n",
                "axes[2].set_title('Feature Correlations with Target', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xlabel('Correlation')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print('\\nüìà Key Statistics:')\n",
                "print(f'   Target Mean: {train_df[TARGET].mean():.2f}')\n",
                "print(f'   Target Std:  {train_df[TARGET].std():.2f}')\n",
                "print(f'   Target Range: [{train_df[TARGET].min():.2f}, {train_df[TARGET].max():.2f}]')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"3\"></a>\n",
                "## 3. Feature Engineering\n",
                "\n",
                "### The \"Magic Formula\"\n",
                "\n",
                "Through careful regression analysis, a powerful linear combination was discovered:\n",
                "\n",
                "$$\\text{Score} \\approx 5.91 \\times \\text{study\\_hours} + 0.35 \\times \\text{attendance} + 1.42 \\times \\text{sleep\\_hours} + 4.78$$\n",
                "\n",
                "### Feature Engineering Strategy\n",
                "\n",
                "1. **Trigonometric Features** - Capture cyclical patterns\n",
                "2. **Frequency Encoding** - Transform categorical counts\n",
                "3. **Polynomial Features** - Log, square, and cube transforms\n",
                "4. **Interaction Terms** - Capture relationships between predictors\n",
                "5. **Ratio Features** - Efficiency metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ADVANCED FEATURE ENGINEERING\n",
                "# ============================================================================\n",
                "\n",
                "NUM_FEATURES = ['study_hours', 'class_attendance', 'sleep_hours', 'age', 'facility_rating']\n",
                "CAT_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']\n",
                "\n",
                "def create_features(df: pd.DataFrame, reference_df: pd.DataFrame = None) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Create comprehensive engineered features.\n",
                "    \"\"\"\n",
                "    result = df.copy()\n",
                "    ref = reference_df if reference_df is not None else df\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 1. THE MAGIC FORMULA (from regression analysis)\n",
                "    # =========================================================================\n",
                "    result['magic_formula'] = (\n",
                "        5.9051154511950499 * result['study_hours'] +\n",
                "        0.34540967058057986 * result['class_attendance'] +\n",
                "        1.423461171860262 * result['sleep_hours'] + \n",
                "        4.7819\n",
                "    )\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 2. TRIGONOMETRIC FEATURES\n",
                "    # =========================================================================\n",
                "    result['study_hours_sin'] = np.sin(2 * np.pi * result['study_hours'] / 12)\n",
                "    result['study_hours_cos'] = np.cos(2 * np.pi * result['study_hours'] / 12)\n",
                "    result['attendance_sin'] = np.sin(2 * np.pi * result['class_attendance'] / 100)\n",
                "    result['attendance_cos'] = np.cos(2 * np.pi * result['class_attendance'] / 100)\n",
                "    result['sleep_sin'] = np.sin(2 * np.pi * result['sleep_hours'] / 12)\n",
                "    result['sleep_cos'] = np.cos(2 * np.pi * result['sleep_hours'] / 12)\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 3. POLYNOMIAL FEATURES\n",
                "    # =========================================================================\n",
                "    for col in ['study_hours', 'class_attendance', 'sleep_hours']:\n",
                "        result[f'{col}_log'] = np.log1p(result[col])\n",
                "        result[f'{col}_sq'] = result[col] ** 2\n",
                "        result[f'{col}_cb'] = result[col] ** 3\n",
                "        result[f'{col}_sqrt'] = np.sqrt(result[col])\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 4. FREQUENCY ENCODING\n",
                "    # =========================================================================\n",
                "    for col in CAT_FEATURES:\n",
                "        freq_map = ref[col].astype(str).value_counts().to_dict()\n",
                "        result[f'{col}_freq'] = result[col].astype(str).map(freq_map).fillna(0).astype(int)\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 5. INTERACTION FEATURES\n",
                "    # =========================================================================\n",
                "    result['study_x_attendance'] = result['study_hours'] * result['class_attendance']\n",
                "    result['study_x_sleep'] = result['study_hours'] * result['sleep_hours']\n",
                "    result['attendance_x_sleep'] = result['class_attendance'] * result['sleep_hours']\n",
                "    result['study_attendance_ratio'] = result['study_hours'] / (result['class_attendance'] + 1)\n",
                "    result['study_efficiency'] = result['study_hours'] * result['class_attendance'] / 100\n",
                "    result['total_effort'] = result['study_hours'] + result['class_attendance'] / 10\n",
                "    result['study_sleep_balance'] = result['study_hours'] / (result['sleep_hours'] + 1)\n",
                "    \n",
                "    # Triple interaction\n",
                "    result['study_att_sleep'] = result['study_hours'] * result['class_attendance'] * result['sleep_hours'] / 1000\n",
                "    \n",
                "    # =========================================================================\n",
                "    # 6. ORDINAL ENCODING\n",
                "    # =========================================================================\n",
                "    ordinal_maps = {\n",
                "        'sleep_quality': {'Poor': 0, 'Average': 1, 'Good': 2},\n",
                "        'exam_difficulty': {'Easy': 0, 'Medium': 1, 'Hard': 2},\n",
                "        'internet_access': {'No': 0, 'Yes': 1}\n",
                "    }\n",
                "    for col, mapping in ordinal_maps.items():\n",
                "        if col in result.columns:\n",
                "            result[f'{col}_ord'] = result[col].map(mapping).fillna(1)\n",
                "    \n",
                "    # Rest quality interaction\n",
                "    if 'sleep_quality_ord' in result.columns:\n",
                "        result['rest_quality'] = result['sleep_hours'] * (result['sleep_quality_ord'] + 1)\n",
                "    \n",
                "    # Difficulty-adjusted study\n",
                "    if 'exam_difficulty_ord' in result.columns:\n",
                "        result['difficulty_adjusted_study'] = result['study_hours'] / (result['exam_difficulty_ord'] + 1)\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Apply feature engineering\n",
                "train_fe = create_features(train_df, train_df)\n",
                "test_fe = create_features(test_df, train_df)\n",
                "original_fe = create_features(original_df, train_df)\n",
                "\n",
                "# Feature columns\n",
                "feature_cols = [col for col in train_fe.columns if col not in CAT_FEATURES + [TARGET, ID_COL]]\n",
                "\n",
                "print(f'‚úì Feature engineering complete: {len(feature_cols)} features')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# PREPROCESSING PIPELINE\n",
                "# ============================================================================\n",
                "\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(train_fe[feature_cols].fillna(0))\n",
                "\n",
                "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
                "encoder.fit(train_fe[CAT_FEATURES].astype(str))\n",
                "\n",
                "def preprocess(df: pd.DataFrame) -> np.ndarray:\n",
                "    nums = scaler.transform(df[feature_cols].fillna(0))\n",
                "    cats = encoder.transform(df[CAT_FEATURES].astype(str))\n",
                "    return np.hstack([nums, cats])\n",
                "\n",
                "X_train = preprocess(train_fe)\n",
                "X_test = preprocess(test_fe)\n",
                "X_original = preprocess(original_fe)\n",
                "\n",
                "print(f'‚úì X_train: {X_train.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"4\"></a>\n",
                "## 4. Model Training Pipeline\n",
                "\n",
                "### Model Selection\n",
                "\n",
                "| Model | Strengths | GPU |\n",
                "|-------|-----------|-----|\n",
                "| Ridge | Stable linear baseline | - |\n",
                "| ElasticNet | Feature selection | - |\n",
                "| BayesianRidge | Uncertainty estimation | - |\n",
                "| ExtraTrees | Random feature sampling | - |\n",
                "| LightGBM | Fast, handles categoricals | CPU |\n",
                "| XGBoost | Robust, well-regularized | ‚úì GPU |\n",
                "| CatBoost | Native categorical handling | ‚úì GPU |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# MODEL HYPERPARAMETERS\n",
                "# ============================================================================\n",
                "\n",
                "# LightGBM (CPU - more stable on Kaggle)\n",
                "LGB_PARAMS = {\n",
                "    'n_estimators': 4000,\n",
                "    'learning_rate': 0.012,\n",
                "    'num_leaves': 255,\n",
                "    'max_depth': 12,\n",
                "    'min_child_samples': 15,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.6,\n",
                "    'reg_alpha': 0.3,\n",
                "    'reg_lambda': 1.5,\n",
                "    'random_state': SEED,\n",
                "    'n_jobs': -1,\n",
                "    'verbose': -1\n",
                "}\n",
                "\n",
                "# XGBoost with GPU (XGBoost 3.1+ syntax)\n",
                "XGB_PARAMS = {\n",
                "    'n_estimators': 4000,\n",
                "    'learning_rate': 0.012,\n",
                "    'max_depth': 10,\n",
                "    'min_child_weight': 5,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.6,\n",
                "    'reg_alpha': 0.3,\n",
                "    'reg_lambda': 1.5,\n",
                "    'random_state': SEED,\n",
                "    'n_jobs': -1,\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda',\n",
                "    'early_stopping_rounds': 400\n",
                "}\n",
                "\n",
                "# CatBoost with GPU\n",
                "CAT_PARAMS = {\n",
                "    'iterations': 4000,\n",
                "    'learning_rate': 0.012,\n",
                "    'depth': 10,\n",
                "    'l2_leaf_reg': 2.0,\n",
                "    'min_data_in_leaf': 15,\n",
                "    'random_seed': SEED,\n",
                "    'verbose': False,\n",
                "    'task_type': 'GPU',\n",
                "    'devices': '0',\n",
                "    'early_stopping_rounds': 400\n",
                "}\n",
                "\n",
                "# ExtraTrees\n",
                "ET_PARAMS = {\n",
                "    'n_estimators': 500,\n",
                "    'max_depth': 20,\n",
                "    'min_samples_split': 10,\n",
                "    'min_samples_leaf': 5,\n",
                "    'random_state': SEED,\n",
                "    'n_jobs': -1\n",
                "}\n",
                "\n",
                "print('‚úì Model configurations ready')\n",
                "print('  - LightGBM: CPU (stable)')\n",
                "print('  - XGBoost: GPU (CUDA)')\n",
                "print('  - CatBoost: GPU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# TRAINING LOOP\n",
                "# ============================================================================\n",
                "\n",
                "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "MODEL_NAMES = ['Ridge', 'ElasticNet', 'BayesianRidge', 'ExtraTrees', 'LightGBM', 'XGBoost', 'CatBoost']\n",
                "n_models = len(MODEL_NAMES)\n",
                "\n",
                "oof_predictions = np.zeros((len(X_train), n_models))\n",
                "test_predictions = np.zeros((len(X_test), n_models))\n",
                "cv_scores = {name: [] for name in MODEL_NAMES}\n",
                "\n",
                "print('=' * 70)\n",
                "print(f'Training {n_models} models with {N_FOLDS}-fold CV')\n",
                "print('=' * 70)\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
                "    print(f'\\n--- Fold {fold + 1}/{N_FOLDS} ---')\n",
                "    \n",
                "    # Augment with original data\n",
                "    X_tr = np.vstack([X_train[train_idx], X_original])\n",
                "    y_tr = np.concatenate([y_train[train_idx], y_original])\n",
                "    X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
                "    \n",
                "    print(f'Train: {len(X_tr):,} | Val: {len(X_val):,}')\n",
                "    \n",
                "    # Model 0: Ridge\n",
                "    ridge = Ridge(alpha=0.1, random_state=SEED)\n",
                "    ridge.fit(X_tr, y_tr)\n",
                "    pred = ridge.predict(X_val)\n",
                "    oof_predictions[val_idx, 0] = pred\n",
                "    test_predictions[:, 0] += ridge.predict(X_test) / N_FOLDS\n",
                "    cv_scores['Ridge'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 1: ElasticNet\n",
                "    enet = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=SEED, max_iter=5000)\n",
                "    enet.fit(X_tr, y_tr)\n",
                "    pred = enet.predict(X_val)\n",
                "    oof_predictions[val_idx, 1] = pred\n",
                "    test_predictions[:, 1] += enet.predict(X_test) / N_FOLDS\n",
                "    cv_scores['ElasticNet'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 2: BayesianRidge\n",
                "    br = BayesianRidge()\n",
                "    br.fit(X_tr, y_tr)\n",
                "    pred = br.predict(X_val)\n",
                "    oof_predictions[val_idx, 2] = pred\n",
                "    test_predictions[:, 2] += br.predict(X_test) / N_FOLDS\n",
                "    cv_scores['BayesianRidge'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 3: ExtraTrees\n",
                "    et = ExtraTreesRegressor(**ET_PARAMS)\n",
                "    et.fit(X_tr, y_tr)\n",
                "    pred = et.predict(X_val)\n",
                "    oof_predictions[val_idx, 3] = pred\n",
                "    test_predictions[:, 3] += et.predict(X_test) / N_FOLDS\n",
                "    cv_scores['ExtraTrees'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 4: LightGBM (CPU)\n",
                "    lgb_model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
                "    lgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(400, verbose=False)])\n",
                "    pred = lgb_model.predict(X_val)\n",
                "    oof_predictions[val_idx, 4] = pred\n",
                "    test_predictions[:, 4] += lgb_model.predict(X_test) / N_FOLDS\n",
                "    cv_scores['LightGBM'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 5: XGBoost (GPU)\n",
                "    xgb_model = xgb.XGBRegressor(**XGB_PARAMS)\n",
                "    xgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
                "    pred = xgb_model.predict(X_val)\n",
                "    oof_predictions[val_idx, 5] = pred\n",
                "    test_predictions[:, 5] += xgb_model.predict(X_test) / N_FOLDS\n",
                "    cv_scores['XGBoost'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Model 6: CatBoost (GPU)\n",
                "    cat_model = CatBoostRegressor(**CAT_PARAMS)\n",
                "    cat_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=False)\n",
                "    pred = cat_model.predict(X_val)\n",
                "    oof_predictions[val_idx, 6] = pred\n",
                "    test_predictions[:, 6] += cat_model.predict(X_test) / N_FOLDS\n",
                "    cv_scores['CatBoost'].append(np.sqrt(mean_squared_error(y_val, pred)))\n",
                "    \n",
                "    # Print summary\n",
                "    scores = [cv_scores[name][-1] for name in MODEL_NAMES[-3:]]\n",
                "    print(f'LGB: {scores[0]:.5f} | XGB: {scores[1]:.5f} | CAT: {scores[2]:.5f}')\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CV Summary\n",
                "print('\\n' + '=' * 70)\n",
                "print('CROSS-VALIDATION SUMMARY')\n",
                "print('=' * 70)\n",
                "for name in MODEL_NAMES:\n",
                "    print(f'{name:14s}: {np.mean(cv_scores[name]):.5f} ¬± {np.std(cv_scores[name]):.5f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"5\"></a>\n",
                "## 5. Advanced Ensemble & Stacking\n",
                "\n",
                "### Hill Climbing Ensemble\n",
                "\n",
                "Inspired by top solutions, we use hill climbing with **negative weights allowed**. This can sometimes improve ensemble performance by using anti-correlated predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ADVANCED WEIGHT OPTIMIZATION (with negative weights)\n",
                "# ============================================================================\n",
                "\n",
                "def ensemble_rmse(weights: np.ndarray) -> float:\n",
                "    prediction = (oof_predictions * weights).sum(axis=1)\n",
                "    return np.sqrt(mean_squared_error(y_train, prediction))\n",
                "\n",
                "# Standard optimization\n",
                "result1 = minimize(ensemble_rmse, [1/n_models]*n_models, bounds=[(0,1)]*n_models, method='SLSQP')\n",
                "weights1 = np.array(result1.x) / sum(result1.x)\n",
                "rmse1 = ensemble_rmse(weights1)\n",
                "\n",
                "# With negative weights allowed (like hill climbing)\n",
                "result2 = minimize(ensemble_rmse, [1/n_models]*n_models, bounds=[(-0.5, 1.5)]*n_models, method='SLSQP')\n",
                "weights2 = result2.x\n",
                "rmse2 = ensemble_rmse(weights2)\n",
                "\n",
                "print('Optimal Weights Comparison:')\n",
                "print('-' * 50)\n",
                "print(f'{\"Model\":14s} | {\"Standard\":>10s} | {\"Hill Climb\":>10s}')\n",
                "print('-' * 50)\n",
                "for i, name in enumerate(MODEL_NAMES):\n",
                "    print(f'{name:14s} | {weights1[i]:>10.4f} | {weights2[i]:>10.4f}')\n",
                "print('-' * 50)\n",
                "print(f'{\"RMSE\":14s} | {rmse1:>10.5f} | {rmse2:>10.5f}')\n",
                "\n",
                "# Use best\n",
                "if rmse2 < rmse1:\n",
                "    optimal_weights = weights2\n",
                "    print('\\n‚úì Using Hill Climbing weights (negative allowed)')\n",
                "else:\n",
                "    optimal_weights = weights1\n",
                "    print('\\n‚úì Using Standard weights')\n",
                "\n",
                "weighted_oof = (oof_predictions * optimal_weights).sum(axis=1)\n",
                "weighted_test = (test_predictions * optimal_weights).sum(axis=1)\n",
                "weighted_rmse = np.sqrt(mean_squared_error(y_train, weighted_oof))\n",
                "print(f'\\nWeighted Ensemble RMSE: {weighted_rmse:.5f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# RIDGE STACKING\n",
                "# ============================================================================\n",
                "\n",
                "print('Training Ridge Stacking...')\n",
                "meta = RidgeCV(alphas=np.logspace(-3, 8, 200), scoring='neg_root_mean_squared_error')\n",
                "meta.fit(oof_predictions, y_train)\n",
                "\n",
                "stacked_oof = meta.predict(oof_predictions)\n",
                "stacked_test = meta.predict(test_predictions)\n",
                "stacked_rmse = np.sqrt(mean_squared_error(y_train, stacked_oof))\n",
                "\n",
                "print(f'Stacking RMSE: {stacked_rmse:.5f} (alpha={meta.alpha_:.2f})')\n",
                "\n",
                "# ============================================================================\n",
                "# BLEND WEIGHTED + STACKED\n",
                "# ============================================================================\n",
                "\n",
                "# Try blending both approaches\n",
                "best_blend_rmse = float('inf')\n",
                "best_blend_ratio = 0\n",
                "\n",
                "for ratio in np.arange(0, 1.01, 0.05):\n",
                "    blended = ratio * weighted_oof + (1 - ratio) * stacked_oof\n",
                "    rmse = np.sqrt(mean_squared_error(y_train, blended))\n",
                "    if rmse < best_blend_rmse:\n",
                "        best_blend_rmse = rmse\n",
                "        best_blend_ratio = ratio\n",
                "\n",
                "print(f'\\nBest Blend: {best_blend_ratio:.0%} weighted + {1-best_blend_ratio:.0%} stacked')\n",
                "print(f'Blended RMSE: {best_blend_rmse:.5f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# FINAL ENSEMBLE SELECTION\n",
                "# ============================================================================\n",
                "\n",
                "print('\\n' + '=' * 70)\n",
                "print('ENSEMBLE COMPARISON')\n",
                "print('=' * 70)\n",
                "print(f'Weighted Average:  {weighted_rmse:.5f}')\n",
                "print(f'Ridge Stacking:    {stacked_rmse:.5f}')\n",
                "print(f'Blended:           {best_blend_rmse:.5f}')\n",
                "\n",
                "# Select best approach\n",
                "results = [\n",
                "    ('Weighted', weighted_rmse, weighted_test),\n",
                "    ('Stacking', stacked_rmse, stacked_test),\n",
                "    ('Blended', best_blend_rmse, best_blend_ratio * weighted_test + (1 - best_blend_ratio) * stacked_test)\n",
                "]\n",
                "\n",
                "best = min(results, key=lambda x: x[1])\n",
                "final_predictions = best[2]\n",
                "final_rmse = best[1]\n",
                "\n",
                "print(f'\\n‚úì Using: {best[0]}')\n",
                "print(f'‚úì Final CV RMSE: {final_rmse:.5f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id=\"6\"></a>\n",
                "## 6. Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CREATE SUBMISSION\n",
                "# ============================================================================\n",
                "\n",
                "final_predictions = np.clip(final_predictions, y_train.min(), y_train.max())\n",
                "\n",
                "submission = pd.DataFrame({'id': test_ids, 'exam_score': final_predictions})\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "\n",
                "print('=' * 70)\n",
                "print('SUBMISSION CREATED')\n",
                "print('=' * 70)\n",
                "print(f'Rows: {len(submission):,}')\n",
                "print(f'Mean: {final_predictions.mean():.2f}')\n",
                "print(f'Std:  {final_predictions.std():.2f}')\n",
                "print(f'Range: [{final_predictions.min():.2f}, {final_predictions.max():.2f}]')\n",
                "print(f'\\nüèÜ Expected LB: ~{final_rmse:.2f}')\n",
                "\n",
                "submission.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "### Techniques Used\n",
                "\n",
                "| Category | Technique |\n",
                "|----------|----------|\n",
                "| Data | Original dataset augmentation |\n",
                "| Features | Magic formula, trigonometric, polynomial, frequency encoding |\n",
                "| Models | Ridge, ElasticNet, BayesianRidge, ExtraTrees, LightGBM, XGBoost, CatBoost |\n",
                "| Ensemble | Hill climbing weights, Ridge stacking, blending |\n",
                "| Optimization | GPU acceleration for XGBoost and CatBoost |\n",
                "\n",
                "### Key Insights\n",
                "\n",
                "1. The relationship is predominantly linear - the \"magic formula\" captures most of the signal\n",
                "2. Model diversity matters more than individual model tuning\n",
                "3. Negative weights in ensemble can improve performance (anti-correlation)\n",
                "4. Original dataset augmentation is crucial for this competition\n",
                "\n",
                "---\n",
                "\n",
                "**If this notebook helped you, please upvote! üëç**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}